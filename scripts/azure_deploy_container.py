#!/usr/bin/env python3
"""Deploy protected-azure-container to Azure Container Instances (ACI).

Model:
- Multi-container group:
  - protected-azure-container app (code-server)
  - Caddy TLS proxy (HTTPS + reverse proxy; Basic Auth)
- Secrets:
  - Full .env is stored as a Key Vault secret (default: 'env')
  - App container fetches env at startup via Managed Identity
  - Basic Auth is configured via ACI secure env vars (recommended)

Usage example:
  python scripts/azure_deploy_container.py \
    --resource-group protected-azure-container-rg \
    --location westeurope \
    --container-name protected-azure-container \
    --image ghcr.io/<you>/protected-azure-container:latest \
    --public-domain yourdomain.com \
    --acme-email you@yourdomain.com \
    --basic-auth-user admin \
    --basic-auth-hash '$2a$14$...' 

Notes:
- Exposes only 80/443 publicly. code-server is behind Caddy at https://<domain>/
- All access is protected by Basic Auth
"""

from __future__ import annotations

import argparse
import json
import math
import os
import sys
import subprocess
import tempfile
import textwrap
import time
from pathlib import Path
from typing import Any

import getpass

from dotenv import load_dotenv

# Add scripts dir to path to allow importing azure_utils
sys.path.append(str(Path(__file__).parent))
try:
    from azure_utils import (
        run_az_command,
        get_service_principal_object_id,
        kv_secret_set_quiet,
        kv_data_plane_available,
    )
except ImportError:
    # Fallback if running from root without scripts in pythonpath
    sys.path.append("scripts")
    from azure_utils import (
        run_az_command,
        get_service_principal_object_id,
        kv_secret_set_quiet,
        kv_data_plane_available,
    )


DEFAULT_APP_PORT = 8080
DEFAULT_CPU_CORES = 1.0
DEFAULT_MEMORY_GB = 2.0

TLS_CPU_CORES = 0.25
TLS_MEMORY_GB = 0.25

DEFAULT_OIDC_APP_NAME = "github-actions-aci-deploy"

DEPLOY_ENV_MATERIALIZE_KEYS = [
    # Azure deploy inputs
    "AZURE_RESOURCE_GROUP",
    "AZURE_LOCATION",
    "AZURE_CONTAINER_NAME",
    "AZURE_DNS_LABEL",
    # Deploy-time routing/TLS
    "AZURE_PUBLIC_DOMAIN",
    "PUBLIC_DOMAIN",
    "AZURE_ACME_EMAIL",
    "ACME_EMAIL",
    # Image / registry
    "GHCR_IMAGE",
    "CONTAINER_IMAGE",
    "IMAGE",
    "GHCR_PRIVATE",
    "GHCR_USERNAME",
    "GHCR_TOKEN",
    "REGISTRY_SERVER",
    "REGISTRY_PASSWORD",
    # Basic Auth
    "BASIC_AUTH_USER",
    "BASIC_AUTH_HASH",
    "BASIC_AUTH_PASSWORD",
    # GitHub Actions Azure OIDC (deploy workflow)
    "AZURE_CLIENT_ID",
    "AZURE_TENANT_ID",
    "AZURE_SUBSCRIPTION_ID",
]


def materialize_deploy_env_file_if_missing(*, path: Path) -> None:
    if path.exists():
        return

    lines: list[str] = [
        "# Auto-generated by scripts/azure_deploy_container.py",
        "# This file is gitignored (.gitignore has .env*).",
        "# It was created because no deploy env file was found, but deploy-time values were present in the environment.",
        "",
    ]

    wrote_any = False
    for key in DEPLOY_ENV_MATERIALIZE_KEYS:
        val = os.getenv(key)
        if val is None:
            continue
        val = str(val)
        if not val.strip():
            continue
        # Basic dotenv formatting; assumes values do not contain newlines.
        lines.append(f"{key}={val}")
        wrote_any = True

    if not wrote_any:
        return

    path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def ensure_oidc_app_and_sp(*, display_name: str) -> str:
    # 1. Try to find existing app
    apps = run_az_command(
        ["ad", "app", "list", "--display-name", display_name, "--query", "[].{appId:appId}", "-o", "json"],
        ignore_errors=True
    )
    app_id: str | None = None
    if apps and isinstance(apps, list) and len(apps) > 0:
        # Prefer exact match if possible (list filtering handles contains sometimes)
        app_id = str(apps[0].get("appId") or "")

    # 2. Create if missing
    if not app_id:
        print(f"[oidc] Creating Azure AD App: {display_name}")
        created = run_az_command(
            ["ad", "app", "create", "--display-name", display_name, "--query", "{appId:appId}", "-o", "json"]
        )
        if isinstance(created, dict):
            app_id = str(created.get("appId") or "")

    if not app_id:
        raise RuntimeError(f"Failed to find or create Azure AD App '{display_name}'")

    # 3. Ensure Service Principal exists (needed for Role Assignments)
    sp_id = run_az_command(
        ["ad", "sp", "show", "--id", app_id, "--query", "id", "-o", "tsv"],
        capture_output=True,
        ignore_errors=True
    )
    if not sp_id:
        print(f"[oidc] Creating Service Principal for App ID: {app_id}")
        run_az_command(["ad", "sp", "create", "--id", app_id], capture_output=False)

    return app_id


def sync_github_actions_vars_secrets(*, repo_root: Path, deploy_env_path: Path | None, azure_client_id: str | None = None) -> None:
    script_path = repo_root / "scripts" / "gh_sync_actions_env.py"
    if not script_path.exists():
        raise SystemExit(f"Missing sync script: {script_path}")

    runtime_env_path = repo_root / ".env"

    cmd = [sys.executable, str(script_path), "--set"]
    if deploy_env_path is not None:
        cmd += ["--deploy-env", str(deploy_env_path)]
    if azure_client_id:
        cmd += ["--azure-client-id", azure_client_id]
    cmd += ["--runtime-env", str(runtime_env_path)]

    try:
        subprocess.run(cmd, check=True)
    except FileNotFoundError:
        raise SystemExit(
            "GitHub CLI (gh) not found. Install and authenticate it, or rerun with --no-set-vars-secrets."
        )
    except subprocess.CalledProcessError as e:
        raise SystemExit(
            f"Failed syncing GitHub Actions vars/secrets (exit={e.returncode}). "
            "Rerun with --no-set-vars-secrets to skip."
        )


def ensure_resource_group(*, resource_group: str, location: str) -> None:
    run_az_command(
        [
            "group",
            "create",
            "--name",
            resource_group,
            "--location",
            location,
            "--output",
            "none",
        ],
        capture_output=False,
    )


def ensure_managed_identity(*, name: str, resource_group: str) -> dict:
    existing = run_az_command(
        ["identity", "show", "--name", name, "--resource-group", resource_group, "--output", "json"],
        ignore_errors=True,
    )
    if isinstance(existing, dict):
        return existing

    created = run_az_command(
        ["identity", "create", "--name", name, "--resource-group", resource_group, "--output", "json"]
    )
    assert isinstance(created, dict)
    return created


def ensure_storage_account(*, name: str, resource_group: str, location: str) -> dict:
    existing = run_az_command(
        ["storage", "account", "show", "--name", name, "--resource-group", resource_group, "--output", "json"],
        ignore_errors=True,
    )
    if isinstance(existing, dict):
        return existing

    created = run_az_command(
        [
            "storage",
            "account",
            "create",
            "--name",
            name,
            "--resource-group",
            resource_group,
            "--location",
            location,
            "--sku",
            "Standard_LRS",
            "--kind",
            "StorageV2",
            "--output",
            "json",
        ]
    )
    assert isinstance(created, dict)
    return created


def ensure_key_vault(*, name: str, resource_group: str, location: str) -> dict:
    existing = run_az_command(["keyvault", "show", "--name", name, "--output", "json"], ignore_errors=True)
    if isinstance(existing, dict):
        return existing

    # Check for soft-deleted vault
    deleted_list = run_az_command(
        ["keyvault", "list-deleted", "--resource-type", "vault", "--query", f"[?name=='{name}']", "-o", "json"],
        ignore_errors=True
    )
    if isinstance(deleted_list, list) and len(deleted_list) > 0:
        print(f"[kv] Vault '{name}' found in soft-deleted state. Recovering...")
        # Note: --resource-group might not be strictly required for recover depending on CLI version, but we include it.
        run_az_command(["keyvault", "recover", "--name", name, "--location", location], capture_output=False)
        
        # Wait for recovery to complete
        print(f"[kv] Waiting for vault '{name}' to recover...")
        for _ in range(12): # Wait up to 60s
            time.sleep(5)
            existing = run_az_command(["keyvault", "show", "--name", name, "--output", "json"], ignore_errors=True)
            if isinstance(existing, dict):
                print(f"[kv] Vault '{name}' recovered successfully.")
                return existing
        print(f"[kv] Timed out waiting for vault '{name}' to recover. Proceeding to create (which may fail)...")

    created = run_az_command(
        [
            "keyvault",
            "create",
            "--name",
            name,
            "--resource-group",
            resource_group,
            "--location",
            location,
            "--enable-rbac-authorization",
            "true",
            "--output",
            "json",
        ]
    )
    assert isinstance(created, dict)
    return created


def ensure_role_assignments(
    *,
    subscription_id: str,
    resource_group: str,
    keyvault_name: str,
    storage_account_name: str,
    identity_principal_id: str,
) -> None:
    # Key Vault Secrets User role for reading the .env secret at runtime.
    kv_scope = (
        f"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/"
        f"Microsoft.KeyVault/vaults/{keyvault_name}"
    )
    run_az_command(
        [
            "role",
            "assignment",
            "create",
            "--role",
            "Key Vault Secrets User",
            "--assignee-object-id",
            identity_principal_id,
            "--assignee-principal-type",
            "ServicePrincipal",
            "--scope",
            kv_scope,
            "--output",
            "none",
        ],
        capture_output=False,
        ignore_errors=True,
    )


def ensure_oidc_app_role_assignment(*, subscription_id: str, resource_group: str, client_id: str, keyvault_name: str, role: str = "Contributor") -> None:
    if not subscription_id or not client_id:
        return
    sp_object_id = get_service_principal_object_id(client_id)
    if not sp_object_id:
        print("[warn] Could not resolve service principal object id for AZURE_CLIENT_ID; skipping role assignment")
        return
    scope = f"/subscriptions/{subscription_id}"
    
    # Assign Contributor (for infra creation) and Key Vault Administrator (for secret management).
    # We assign both at subscription scope to ensure the OIDC SP handles all resource lifecycle events.
    # 'role' defaults to 'Contributor'.
    roles_to_assign = sorted(list({role, "Key Vault Administrator"}))
    for r in roles_to_assign:
        run_az_command(
            [
                "role",
                "assignment",
                "create",
                "--role",
                r,
                "--assignee-object-id",
                sp_object_id,
                "--assignee-principal-type",
                "ServicePrincipal",
                "--scope",
                scope,
                "--output",
                "none",
            ],
            capture_output=False,
            ignore_errors=True,
        )

    # Storage role so the managed identity can access Azure Files data-plane if needed.
    # NOTE: ACI Azure Files volume mounts still require a storage key/SAS in the container group spec.
    # storage_scope uses resource_group, which must be passed in.
    # We find storage accounts in this RG.
    storage_accounts = run_az_command(
        ["storage", "account", "list", "--resource-group", resource_group, "--query", "[].name", "-o", "tsv"],
        capture_output=True,
        ignore_errors=True
    )
    if isinstance(storage_accounts, str):
        for sa_name in storage_accounts.splitlines():
            sa_name = sa_name.strip()
            if not sa_name:
                continue
            storage_scope = (
                f"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/"
                f"Microsoft.Storage/storageAccounts/{sa_name}"
            )
            run_az_command(
                [
                    "role",
                    "assignment",
                    "create",
                    "--role",
                    "Storage File Data SMB Share Contributor",
                    "--assignee-object-id",
                    sp_object_id,
                    "--assignee-principal-type",
                    "ServicePrincipal",
                    "--scope",
                    storage_scope,
                    "--output",
                    "none",
                ],
                capture_output=False,
                ignore_errors=True,
            )

    # Key Vault Secrets Officer role so the OIDC SP can write secrets (env, etc.)
    kv_scope = (
        f"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/"
        f"Microsoft.KeyVault/vaults/{keyvault_name}"
    )
    
    # Also assign "Key Vault Administrator" to be absolutely sure we have management rights
    # Some setups might require this for certain operations.
    for r in ["Key Vault Secrets Officer", "Key Vault Administrator"]:
        run_az_command(
            [
                "role",
                "assignment",
                "create",
                "--role",
                r,
                "--assignee-object-id",
                sp_object_id,
                "--assignee-principal-type",
                "ServicePrincipal",
                "--scope",
                kv_scope,
                "--output",
                "none",
            ],
            capture_output=False,
            ignore_errors=True,
        )


def ensure_infra(
    *,
    resource_group: str,
    location: str,
    container_name: str,
    identity_name: str,
    keyvault_name: str,
    storage_name: str,
    shares: list[str],
) -> None:
    # Keep this best-effort and idempotent; az commands are safe to rerun.
    ensure_resource_group(resource_group=resource_group, location=location)
    identity = ensure_managed_identity(name=identity_name, resource_group=resource_group)
    ensure_storage_account(name=storage_name, resource_group=resource_group, location=location)
    ensure_key_vault(name=keyvault_name, resource_group=resource_group, location=location)

    subscription_id = str(
        run_az_command(["account", "show", "--query", "id", "-o", "tsv"], capture_output=True)
    ).strip()

    principal_id = str(identity.get("principalId") or "").strip()
    if not principal_id:
        raise RuntimeError("Managed Identity principalId was not returned by Azure CLI")

    ensure_role_assignments(
        subscription_id=subscription_id,
        resource_group=resource_group,
        keyvault_name=keyvault_name,
        storage_account_name=storage_name,
        identity_principal_id=principal_id,
    )

    for share in shares:
        ensure_file_share_exists(storage_name, share, resource_group)


def normalize_aci_memory_gb(memory_gb: float) -> float:
    if memory_gb <= 0:
        raise ValueError(f"memory_gb must be > 0, got {memory_gb!r}")
    return math.ceil(memory_gb * 10) / 10


def run_cmd(cmd: list[str], *, cwd: str | None = None, input_text: str | None = None) -> None:
    # Avoid leaking secrets; callers must pass input via input_text.
    print(f"[run] {' '.join(cmd)}")
    subprocess.run(
        cmd,
        cwd=cwd,
        input=input_text,
        text=True if input_text is not None else False,
        check=True,
    )


def docker_pull(*, image: str) -> None:
    run_cmd(["docker", "pull", image])





def _env_filtered_content(*, env_path: Path, prefixes: list[str], raw: bool) -> str:
    """Read a dotenv file and return content suitable for Key Vault secret 'env'.

    By default we only include keys starting with the provided prefixes (e.g. BASIC_AUTH_)
    to avoid accidentally uploading deploy-only secrets (GHCR_TOKEN, etc).
    """
    if not env_path.exists():
        raise SystemExit(f"Env file not found: {env_path}")

    text = env_path.read_text()
    if raw:
        return text

    keep_prefixes = [p for p in (prefixes or []) if (p or "").strip()]
    if not keep_prefixes:
        keep_prefixes = ["BASIC_AUTH_"]

    kept: list[str] = []
    for line in text.splitlines():
        s = line.strip()
        if not s or s.startswith("#"):
            continue

        if "=" not in s:
            continue

        key = s.split("=", 1)[0].strip()
        if any(key.startswith(p) for p in keep_prefixes):
            kept.append(line)

    header = "\n".join(
        [
            "# Generated by scripts/azure_deploy_container.py",
            "# Runtime env uploaded to Key Vault secret 'env'",
            f"# Included prefixes: {', '.join(keep_prefixes)}",
            "",
        ]
    )
    return header + "\n".join(kept) + ("\n" if kept else "")


def parse_image_ref(image: str) -> tuple[str | None, str]:
    """Return (registry, remainder).

    - ghcr.io/owner/repo:tag -> ("ghcr.io", "owner/repo:tag")
    - owner/repo:tag -> (None, "owner/repo:tag")
    """
    image = (image or "").strip()
    if not image:
        return None, ""

    # Docker treats the first segment as a registry if it contains a '.' or ':' or is 'localhost'.
    first = image.split("/", 1)[0]
    if "." in first or ":" in first or first == "localhost":
        registry, rest = image.split("/", 1)
        return registry, rest
    return None, image


def docker_login(*, registry: str, username: str, token: str) -> None:
    registry = (registry or "").strip()
    username = (username or "").strip()
    token = (token or "").strip()
    if not (registry and username and token):
        raise ValueError("registry/username/token must be set for docker login")
    # Use --password-stdin to avoid leaking token.
    print(f"[docker] login {registry} as {username} (token via stdin)")
    run_cmd(["docker", "login", registry, "-u", username, "--password-stdin"], input_text=token + "\n")


def docker_build(*, image: str, context_dir: str, dockerfile: str | None = None) -> None:
    cmd = ["docker", "build", "-t", image]
    if dockerfile:
        cmd += ["-f", dockerfile]
    cmd += [context_dir]
    run_cmd(cmd)


def docker_push(*, image: str) -> None:
    run_cmd(["docker", "push", image])


def _hint_for_ghcr_scope_error(stderr: str | None) -> str | None:
    s = (stderr or "").lower()
    if "expected scopes" in s or "permission_denied" in s or "denied" in s:
        return textwrap.dedent(
            """
            GHCR rejected the push due to insufficient token permissions.

            What you need:
            - For pushing to GHCR: a token with packages write permission.
              - Classic PAT: enable `write:packages` (and often `read:packages`).
              - If the repo/package is private, you may also need `repo`.

            Notes:
            - `docker login` can succeed even if the token cannot push.
            - If you use an org with SSO, you may need to authorize the token for SSO.
            """
        ).strip()
    return None


def is_interactive() -> bool:
    try:
        return sys.stdin.isatty()
    except Exception:
        return False


def az_logged_in() -> bool:
    try:
        run_az_command(["account", "show", "--output", "none"], capture_output=False)
        return True
    except Exception:
        return False


def kv_secret_get(vault_name: str, secret_name: str) -> str | None:
    if not vault_name or not secret_name:
        return None

    try:
        out = run_az_command(
            [
                "keyvault",
                "secret",
                "show",
                "--vault-name",
                vault_name,
                "--name",
                secret_name,
                "--query",
                "value",
                "-o",
                "tsv",
            ],
            capture_output=True,
            ignore_errors=True,
        )
    except Exception:
        return None

    if out is None:
        return None
    if isinstance(out, str):
        v = out.strip()
        return v or None
    return None





def kv_secret_set(vault_name: str, secret_name: str, value: str) -> None:
    run_az_command(
        [
            "keyvault",
            "secret",
            "set",
            "--vault-name",
            vault_name,
            "--name",
            secret_name,
            "--value",
            value,
            "--output",
            "none",
        ],
        capture_output=False,
    )





def prompt_value(label: str, *, default: str | None = None) -> str:
    prompt = f"{label}"
    if default:
        prompt += f" [{default}]"
    prompt += ": "
    v = input(prompt).strip()
    if not v and default is not None:
        return default
    return v


def prompt_secret(label: str) -> str:
    return getpass.getpass(f"{label}: ").strip()


def prompt_yes_no(label: str, *, default: bool = False) -> bool:
    suffix = "[Y/n]" if default else "[y/N]"
    v = input(f"{label} {suffix}: ").strip().lower()
    if not v:
        return default
    return v in {"y", "yes", "true", "1"}


def truthy(value: str | None) -> bool:
    if value is None:
        return False
    return value.strip().lower() not in {"", "0", "false", "no", "off"}


def looks_like_bcrypt_hash(value: str) -> bool:
    v = (value or "").strip()
    # Caddy basicauth bcrypt hashes typically start with $2a$, $2b$, $2y$.
    return v.startswith("$2")


def bcrypt_hash_password(password: str, *, cost: int = 14) -> str:
    password = (password or "").strip()
    if not password:
        raise ValueError("password must be non-empty")
    if cost < 4 or cost > 31:
        raise ValueError("bcrypt cost must be between 4 and 31")

    try:
        import bcrypt  # type: ignore
    except Exception as e:
        raise RuntimeError(
            "Missing dependency 'bcrypt'. Install it with: pip install bcrypt"
        ) from e

    salt = bcrypt.gensalt(rounds=cost)
    hashed = bcrypt.hashpw(password.encode("utf-8"), salt)
    return hashed.decode("utf-8")


def resolve_value(
    *,
    name: str,
    arg_value: str | None,
    env_names: list[str],
    kv_name: str,
    kv_secret_name: str | None,
    interactive: bool,
    secret: bool = False,
    prompt_label: str | None = None,
    default: str | None = None,
    persist_to_kv: bool = False,
) -> str | None:
    if arg_value is not None:
        v = str(arg_value).strip()
        if v:
            return v

    for env_name in env_names:
        v = (os.getenv(env_name) or "").strip()
        if v:
            return v

    if kv_secret_name:
        v = kv_secret_get(kv_name, kv_secret_name)
        if v:
            return v

    if interactive:
        label = prompt_label or name
        v = prompt_secret(label) if secret else prompt_value(label, default=default)
        if v:
            if persist_to_kv and kv_secret_name:
                if prompt_yes_no(f"Save {name} to Key Vault secret '{kv_secret_name}'?", default=True):
                    try:
                        kv_secret_set(kv_name, kv_secret_name, v)
                    except subprocess.CalledProcessError as e:
                        print(
                            "WARNING: Failed to save value to Key Vault; continuing without persisting.",
                            file=sys.stderr,
                        )
                        print(
                            _format_keyvault_set_help(vault_name=kv_name, stderr=getattr(e, "stderr", None)),
                            file=sys.stderr,
                        )
            return v

    return None


def get_storage_key(account_name: str, resource_group: str) -> str:
    keys = run_az_command([
        "storage",
        "account",
        "keys",
        "list",
        "--account-name",
        account_name,
        "--resource-group",
        resource_group,
        "--output",
        "json",
    ])
    if not isinstance(keys, list) or not keys:
        raise RuntimeError("Could not fetch storage account keys (unexpected az output)")

    first: Any = keys[0]
    if not isinstance(first, dict) or "value" not in first:
        raise RuntimeError("Could not parse storage key from az output")

    return str(first["value"])


def get_identity_details(name: str, resource_group: str) -> tuple[str, str | None, str | None]:
    identity = run_az_command([
        "identity",
        "show",
        "--name",
        name,
        "--resource-group",
        resource_group,
        "--output",
        "json",
    ])
    assert isinstance(identity, dict)
    return str(identity["id"]), identity.get("clientId"), identity.get("tenantId")


def ensure_file_share_exists(account_name: str, share_name: str, resource_group: str) -> None:
    run_az_command(
        [
            "storage",
            "share-rm",
            "create",
            "--storage-account",
            account_name,
            "--resource-group",
            resource_group,
            "--name",
            share_name,
            "--quota",
            "5",
            "--output",
            "none",
        ],
        capture_output=False,
        ignore_errors=True,
    )


def generate_deploy_yaml(
    *,
    name: str,
    location: str,
    image: str,
    registry_server: str | None,
    registry_username: str | None,
    registry_password: str | None,
    identity_id: str,
    identity_client_id: str | None,
    identity_tenant_id: str | None,
    storage_name: str,
    storage_key: str,
    kv_name: str,
    dns_label: str,
    public_domain: str,

    acme_email: str,
    basic_auth_user: str,
    basic_auth_hash: str,
    cpu_cores: float,
    memory_gb: float,
    share_workspace: str,
    caddy_data_share_name: str,
    caddy_config_share_name: str,
    caddy_image: str,
) -> str:
    app_memory_gb = normalize_aci_memory_gb(memory_gb)
    tls_memory_gb = normalize_aci_memory_gb(TLS_MEMORY_GB)

    def indent(level: int, text: str) -> str:
        return " " * level + text

    # Caddyfile generated inline
    caddy_cmd = "\n".join(
        [
            "set -eu",
            "mkdir -p /config/caddy",
            "cat > /config/caddy/Caddyfile <<'CADDY'",
            "{",
            "  email {$ACME_EMAIL}",
            "}",
            "",
            f"{public_domain} {{",
            "  encode zstd gzip",
            "  header Strict-Transport-Security \"max-age=31536000; includeSubDomains; preload\"",

            "",
            "  # Single Basic Auth layer for all routes",
            "  basicauth /* {",
            "    {$BASIC_AUTH_USER} {$BASIC_AUTH_HASH}",
            "  }",
            "",
            "  # Proxy to code-server",
            "  reverse_proxy http://localhost:8080 {",
            "    header_up Upgrade {http.request.header.Upgrade}",
            "    header_up Connection {http.request.header.Connection}",
            "  }",
            "}",
            "CADDY",
            "",
            "# Run Caddy with the mounted /data for certs",
            "exec caddy run --config /config/caddy/Caddyfile --adapter caddyfile",
        ]
    )

    lines: list[str] = [
        "apiVersion: '2023-05-01'",
        f"location: {location}",
        f"name: {name}",
        "identity:",
        indent(2, "type: UserAssigned"),
        indent(2, "userAssignedIdentities:"),
        indent(4, f"'{identity_id}': {{}}"),
        "properties:",
    ]

    if registry_server or registry_username or registry_password:
        if not (registry_server and registry_username and registry_password):
            raise ValueError("registry_server/registry_username/registry_password must all be set when using registry credentials")

        lines += [
            indent(2, "imageRegistryCredentials:"),
            indent(4, f"- server: {registry_server}"),
            indent(6, f"username: {registry_username}"),
            indent(6, f"password: '{registry_password}'"),
        ]

    lines += [
        indent(2, "containers:"),
        indent(4, f"- name: {name}"),
        indent(6, "properties:"),
        indent(8, f"image: {image}"),
        indent(8, "ports:"),
        indent(10, f"- port: {DEFAULT_APP_PORT}"),  # 8080 for code-server
        indent(12, "protocol: TCP"),
        indent(8, "resources:"),
        indent(10, "requests:"),
        indent(12, f"cpu: {cpu_cores}"),
        indent(12, f"memoryInGB: {app_memory_gb}"),
        indent(8, "environmentVariables:"),
        indent(10, "- name: CODE_SERVER_PORT"),
        indent(12, "value: '8080'"),
        indent(10, "- name: AZURE_KEYVAULT_URI"),
        indent(12, f"value: 'https://{kv_name}.vault.azure.net/'"),
    ]

    if identity_client_id:
        lines += [
            indent(10, "- name: AZURE_CLIENT_ID"),
            indent(12, f"value: '{identity_client_id}'"),
        ]
    if identity_tenant_id:
        lines += [
            indent(10, "- name: AZURE_TENANT_ID"),
            indent(12, f"value: '{identity_tenant_id}'"),
        ]

    # Entrypoint: default from Dockerfile (/usr/local/bin/azure_start.sh)
    lines += [
        indent(8, "volumeMounts:"),
        indent(10, "- name: workspace-volume"),
        indent(12, "mountPath: /home/coder/workspace"),
        "",
        indent(4, "- name: tls-proxy"),
        indent(6, "properties:"),
        indent(8, f"image: {caddy_image}"),
        indent(8, "ports:"),
        indent(10, "- port: 80"),
        indent(12, "protocol: TCP"),
        indent(10, "- port: 443"),
        indent(12, "protocol: TCP"),
        indent(8, "resources:"),
        indent(10, "requests:"),
        indent(12, f"cpu: {TLS_CPU_CORES}"),
        indent(12, f"memoryInGB: {tls_memory_gb}"),
        indent(8, "environmentVariables:"),
        indent(10, "- name: PUBLIC_DOMAIN"),
        indent(12, f"value: '{public_domain}'"),
        indent(10, "- name: ACME_EMAIL"),
        indent(12, f"value: '{acme_email}'"),
        indent(10, "- name: FALLBACK_DOMAIN"),
        indent(12, f"value: '{dns_label}.{location}.azurecontainer.io'"),
        indent(10, "- name: BASIC_AUTH_USER"),
        indent(12, f"value: '{basic_auth_user}'"),
        indent(10, "- name: BASIC_AUTH_HASH"),
        indent(12, f"secureValue: '{basic_auth_hash}'"),
        indent(8, "command:"),
        indent(10, "- sh"),
        indent(10, "- -lc"),
        indent(10, "- |"),
    ]

    for line in caddy_cmd.splitlines():
        lines.append(indent(12, line))

    lines += [
        indent(8, "volumeMounts:"),
        indent(10, "- name: caddy-data"),
        indent(12, "mountPath: /data"),
        indent(10, "- name: caddy-config"),
        indent(12, "mountPath: /config"),
        "",
        indent(2, "osType: Linux"),
        indent(2, "restartPolicy: Always"),
        indent(2, "ipAddress:"),
        indent(4, "type: Public"),
        indent(4, f"dnsNameLabel: {dns_label}"),
        indent(4, "ports:"),
        indent(6, "- port: 80"),
        indent(6, "- port: 443"),
        "",
        indent(2, "volumes:"),
        indent(4, "- name: workspace-volume"),
        indent(6, "azureFile:"),
        indent(8, f"shareName: {share_workspace}"),
        indent(8, f"storageAccountName: {storage_name}"),
        indent(8, f"storageAccountKey: {storage_key}"),
        indent(4, "- name: caddy-data"),
        indent(6, "azureFile:"),
        indent(8, f"shareName: {caddy_data_share_name}"),
        indent(8, f"storageAccountName: {storage_name}"),
        indent(8, f"storageAccountKey: {storage_key}"),
        indent(4, "- name: caddy-config"),
        indent(6, "azureFile:"),
        indent(8, f"shareName: {caddy_config_share_name}"),
        indent(8, f"storageAccountName: {storage_name}"),
        indent(8, f"storageAccountKey: {storage_key}"),
    ]

    return "\n".join(lines) + "\n"


def main() -> None:
    parser = argparse.ArgumentParser(description="Deploy protected-azure-container to Azure Container Instances")

    # These can come from --env-file (recommended) so they are not required.
    parser.add_argument("--resource-group", "-g", required=False, default=None)
    parser.add_argument("--location", "-l", default=None)
    parser.add_argument("--container-name", "-n", default=None)
    parser.add_argument("--dns-label", default=None, help="DNS label for <label>.<location>.azurecontainer.io")

    parser.add_argument("--image", "-i", default=None, help="Container image URL")

    parser.add_argument("--storage-name", default=None)
    parser.add_argument("--identity-name", default=None)
    parser.add_argument("--keyvault-name", default=None)

    parser.add_argument("--share-workspace", default=None)
    parser.add_argument("--caddy-data-share-name", default=None)
    parser.add_argument("--caddy-config-share-name", default=None)

    parser.add_argument("--public-domain", default=None)
    parser.add_argument("--acme-email", default=None)

    parser.add_argument("--basic-auth-user", default=None)
    parser.add_argument(
        "--basic-auth-hash",
        default=None,
        help="Basic Auth bcrypt hash. If you pass a plain password instead, the script will compute the bcrypt hash automatically.",
    )
    parser.add_argument(
        "--basic-auth-password",
        default=None,
        help="Basic Auth password (used to compute bcrypt hash if --basic-auth-hash not provided)",
    )
    parser.add_argument(
        "--bcrypt-cost",
        type=int,
        default=14,
        help="bcrypt cost for generated hash (default: 14)",
    )

    parser.add_argument("--registry-server", default=None, help="Container registry server (e.g. ghcr.io)")
    parser.add_argument("--registry-username", default=None, help="Registry username (for private images)")
    parser.add_argument("--registry-password", default=None, help="Registry password/token (for private images)")

    parser.add_argument(
        "--build",
        action="store_true",
        help="Build the container image locally before deploy (docker build)",
    )
    parser.add_argument(
        "--push",
        action="store_true",
        help="Push the container image before deploy (docker push)",
    )
    parser.add_argument(
        "--build-push",
        action="store_true",
        help="Build and push the container image before deploy",
    )

    parser.add_argument(
        "--publish",
        action=argparse.BooleanOptionalAction,
        default=None,
        help="Build + push the image before deploy (default: enabled when GHCR_PRIVATE=true)",
    )
    parser.add_argument(
        "--docker-context",
        default=None,
        help="Docker build context directory (default: repo root)",
    )
    parser.add_argument(
        "--dockerfile",
        default=None,
        help="Optional Dockerfile path (default: use Docker's default resolution)",
    )

    parser.add_argument("--interactive", action=argparse.BooleanOptionalAction, default=None)
    parser.add_argument(
        "--persist-to-keyvault",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Offer to save entered deploy-time values to Key Vault secrets (default: off)",
    )

    parser.add_argument("--public-domain-secret", default="public-domain")
    parser.add_argument("--acme-email-secret", default="acme-email")
    parser.add_argument("--basic-auth-user-secret", default="basic-auth-user")
    parser.add_argument("--basic-auth-hash-secret", default="basic-auth-hash")
    parser.add_argument("--image-secret", default="image")
    parser.add_argument("--registry-username-secret", default="registry-username")
    parser.add_argument("--registry-password-secret", default="registry-password")
    parser.add_argument("--registry-server-secret", default="registry-server")

    parser.add_argument(
        "--env-file",
        default=None,
        help=(
            "Env file to load for deploy-time values. If omitted, auto-detects (in order): "
            ".env.deploy, env.deploy, .env"
        ),
    )

    parser.add_argument(
        "--azure-oidc-app-name",
        default=DEFAULT_OIDC_APP_NAME,
        help=f"Azure AD App Registration name for GitHub Actions OIDC (default: {DEFAULT_OIDC_APP_NAME})",
    )

    parser.add_argument(
        "--set-vars-secrets",
        action=argparse.BooleanOptionalAction,
        default=True,
        help=(
            "Sync deploy-time vars/secrets to GitHub Actions (via scripts/gh_sync_actions_env.py). "
            "Default: enabled. Use --no-set-vars-secrets in CI."
        ),
    )

    parser.add_argument(
        "--upload-env",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Upload runtime env to Key Vault secret 'env' before deploy (default: enabled)",
    )
    parser.add_argument(
        "--upload-env-file",
        default=None,
        help="Path to runtime env file to upload (default: repo root .env)",
    )
    parser.add_argument(
        "--upload-env-secret-name",
        default="env",
        help="Key Vault secret name for runtime env (default: env)",
    )
    parser.add_argument(
        "--upload-env-prefixes",
        default="BASIC_AUTH_",
        help="Comma-separated prefixes to include when uploading env (default: BASIC_AUTH_)",
    )
    parser.add_argument(
        "--upload-env-raw",
        action="store_true",
        help="Upload the full env file content (DANGER: may include deploy-only secrets)",
    )

    parser.add_argument(
        "--prefetch-images",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Pre-pull Caddy image locally to validate it exists (default: enabled)",
    )

    parser.add_argument("--cpu", type=float, default=DEFAULT_CPU_CORES)
    parser.add_argument("--memory", type=float, default=DEFAULT_MEMORY_GB)

    # Prefer a stable mirror to avoid Docker Hub rate limiting in ACI.
    # Note: ghcr.io/caddyserver/caddy does not publish a '2-alpine' tag; we use a mirror.
    parser.add_argument("--caddy-image", default="caddy:2-alpine")

    args = parser.parse_args()

    repo_root = Path(__file__).resolve().parents[1]

    interactive = is_interactive() if args.interactive is None else bool(args.interactive)
    # Key Vault is used at *runtime* by the container to fetch the full .env secret.
    # For deploy-time inputs (image/domain/registry creds), default to local env/args.
    persist_to_kv = bool(args.persist_to_keyvault)

    # Load deploy-time values.
    # We load .env (runtime config) first, then .env.deploy (deploy-time overrides) on top.
    
    # 1. Load .env (if exists)
    runtime_env_path = repo_root / ".env"
    if runtime_env_path.exists():
        load_dotenv(dotenv_path=str(runtime_env_path), override=False)

    # 2. Load .env.deploy (or custom file)
    deploy_env_path: Path | None = None
    if args.env_file:
        deploy_env_path = Path(args.env_file).expanduser().resolve()
    else:
        for candidate in (repo_root / ".env.deploy", repo_root / "env.deploy"):
            if candidate.exists():
                deploy_env_path = candidate
                break

    # If no deploy env file exists, but we are defaulting, use .env.deploy path for materialization
    if deploy_env_path is None:
        deploy_env_path = repo_root / ".env.deploy"
        
    # load_dotenv() is a no-op if the file doesn't exist.
    # override=True ensures deploy-time vars take precedence over runtime vars.
    if deploy_env_path is not None and deploy_env_path.exists():
        load_dotenv(dotenv_path=str(deploy_env_path), override=True)
    elif not runtime_env_path.exists():
        # If neither exists, materialize .env.deploy if we have env vars (CI case)
        materialize_deploy_env_file_if_missing(path=deploy_env_path)

    if not az_logged_in():
        raise SystemExit("Not logged into Azure. Run: az login")

    # Resolve Azure OIDC App (create if missing) so sync script has correct ID.
    # Prioritize: Env EnvVar -> Arg Default -> Lookup/Create
    oidc_client_id = (os.getenv("AZURE_CLIENT_ID") or os.getenv("AZURE_APP_ID") or "").strip()
    if not oidc_client_id and bool(args.set_vars_secrets):
        oidc_app_name = (args.azure_oidc_app_name or DEFAULT_OIDC_APP_NAME).strip()
        oidc_client_id = ensure_oidc_app_and_sp(display_name=oidc_app_name)
        # Set in env so downstream logic can use it
        os.environ["AZURE_CLIENT_ID"] = oidc_client_id

    # Keep GitHub Actions vars/secrets in sync by default (so CI has what it needs).
    # In CI, pass --no-set-vars-secrets.
    if bool(args.set_vars_secrets):
        sync_github_actions_vars_secrets(repo_root=repo_root, deploy_env_path=deploy_env_path, azure_client_id=oidc_client_id)

    rg = (args.resource_group or os.getenv("AZURE_RESOURCE_GROUP") or os.getenv("RESOURCE_GROUP") or "").strip()
    if not rg:
        raise SystemExit(
            "Missing resource group. Provide --resource-group, or set AZURE_RESOURCE_GROUP in env.deploy/.env.deploy (or pass --env-file)."
        )

    location = (args.location or os.getenv("AZURE_LOCATION") or os.getenv("LOCATION") or "westeurope").strip() or "westeurope"
    name = (args.container_name or os.getenv("AZURE_CONTAINER_NAME") or os.getenv("ACI_CONTAINER_NAME") or "protected-azure-container").strip() or "protected-azure-container"
    dns_label = (args.dns_label or name).strip().lower()

    storage_name = (args.storage_name or f"{rg}stg").replace("-", "")
    storage_name = "".join([c for c in storage_name.lower() if c.isalnum()])[:24]

    # Sanitize Key Vault name: <24 chars, alphanumeric/hyphens, no start/end hyphen.
    # Default: derived from RG name.
    # We strip hyphens to save space and reduce risk of consecutive hyphens.
    identity_name = args.identity_name or f"{rg}-identity"

    if args.keyvault_name:
        kv_name = args.keyvault_name
    else:
        # e.g. "protected-azure-container-rg" -> "protectedazurecontainkv"
        base = "".join([c for c in rg.lower() if c.isalnum()])
        kv_name = f"{base}kv"[:24]

    # Ensure Azure resources exist so a single azure_deploy_container invocation can bootstrap infra.
    shares_to_ensure = [
        args.share_workspace or f"{name}-workspace",
        args.caddy_data_share_name or f"{name}-caddy-data",
        args.caddy_config_share_name or f"{name}-caddy-config",
    ]
    ensure_infra(
        resource_group=rg,
        location=location,
        container_name=name,
        identity_name=identity_name,
        keyvault_name=kv_name,
        storage_name=storage_name,
        shares=shares_to_ensure,
    )

    subscription_id = (os.getenv("AZURE_SUBSCRIPTION_ID") or "").strip()
    if not subscription_id:
        subscription_id = str(
            run_az_command(["account", "show", "--query", "id", "-o", "tsv"], capture_output=True)
        ).strip()
    if oidc_client_id and subscription_id:
        ensure_oidc_app_role_assignment(
            subscription_id=subscription_id, 
            resource_group=rg, 
            client_id=oidc_client_id,
            keyvault_name=kv_name
        )

    # Upload runtime env to Key Vault for the container to fetch at startup.
    # By default we upload only BASIC_AUTH_* keys to avoid leaking deploy credentials.
    if args.upload_env:
        # By default, always upload the repo root .env (runtime) so KV has the latest
        # runtime configuration, even if deploy-time values are loaded from .env.deploy.
        default_runtime_env = repo_root / ".env"
        upload_env_path = Path(args.upload_env_file).resolve() if args.upload_env_file else default_runtime_env

        # Safety: never upload deploy-only env files to Key Vault.
        if upload_env_path.name in {"env.deploy", ".env.deploy"}:
            raise SystemExit(
                f"Refusing to upload deploy-only env file to Key Vault: {upload_env_path}. "
                "Put runtime settings in .env (repo root) or pass --upload-env-file <runtime_env>."
            )
        if not upload_env_path.exists():
            raise SystemExit(
                f"Runtime env file not found: {upload_env_path}. "
                "Create .env (runtime) or pass --no-upload-env if you want to deploy without uploading."
            )

        prefixes = [p.strip() for p in (args.upload_env_prefixes or "").split(",") if p.strip()]
        try:
            env_content = _env_filtered_content(env_path=upload_env_path, prefixes=prefixes, raw=bool(args.upload_env_raw))
            kv_secret_set_quiet(vault_name=kv_name, secret_name=str(args.upload_env_secret_name), value=env_content)
        except subprocess.CalledProcessError as e:
            print(_format_keyvault_set_help(vault_name=kv_name, stderr=getattr(e, "stderr", None)), file=sys.stderr)
            raise SystemExit(1)

    kv_name_for_secrets = ""
    if persist_to_kv:
        kv_ok = kv_data_plane_available(kv_name)
        if not kv_ok:
            print("[deploy] Disabling --persist-to-keyvault because Key Vault is not reachable.", file=sys.stderr)
            persist_to_kv = False
        else:
            kv_name_for_secrets = kv_name

    share_workspace = shares_to_ensure[0]
    caddy_data_share = shares_to_ensure[1]
    caddy_config_share = shares_to_ensure[2]

    image = resolve_value(
        name="image",
        arg_value=args.image,
        env_names=["CONTAINER_IMAGE", "IMAGE", "GHCR_IMAGE"],
        kv_name=kv_name_for_secrets,
        kv_secret_name=args.image_secret,
        interactive=interactive,
        secret=False,
        prompt_label="Container image (e.g. ghcr.io/<owner>/protected-azure-container:tag)",
        persist_to_kv=persist_to_kv,
    )
    if not image:
        raise SystemExit(
            "Missing container image. Provide --image, set GHCR_IMAGE, or store Key Vault secret 'image'."
        )

    # Resolve build/push mode.
    build_requested = bool(args.build or args.build_push)
    push_requested = bool(args.push or args.build_push)

    public_domain = resolve_value(
        name="public_domain",
        arg_value=args.public_domain,
        env_names=["PUBLIC_DOMAIN", "AZURE_PUBLIC_DOMAIN"],
        kv_name=kv_name_for_secrets,
        kv_secret_name=args.public_domain_secret,
        interactive=interactive,
        secret=False,
        prompt_label="Public domain (e.g. yourdomain.com)",
        persist_to_kv=persist_to_kv,
    )
    if not public_domain:
        raise SystemExit(
            "Missing public domain. Provide --public-domain, set AZURE_PUBLIC_DOMAIN, or store Key Vault secret 'public-domain'."
        )

    acme_email = resolve_value(
        name="acme_email",
        arg_value=args.acme_email,
        env_names=["ACME_EMAIL", "AZURE_ACME_EMAIL"],
        kv_name=kv_name_for_secrets,
        kv_secret_name=args.acme_email_secret,
        interactive=interactive,
        secret=False,
        prompt_label="ACME email (Let's Encrypt)",
        persist_to_kv=persist_to_kv,
    )
    if not acme_email:
        raise SystemExit(
            "Missing ACME email. Provide --acme-email, set AZURE_ACME_EMAIL, or store Key Vault secret 'acme-email'."
        )

    basic_auth_user = resolve_value(
        name="basic_auth_user",
        arg_value=args.basic_auth_user,
        env_names=["BASIC_AUTH_USER"],
        kv_name=kv_name_for_secrets,
        kv_secret_name=args.basic_auth_user_secret,
        interactive=interactive,
        secret=False,
        prompt_label="Basic Auth username",
        default="admin",
        persist_to_kv=persist_to_kv,
    ) or "admin"

    # Only resolve an existing hash from args/env/Key Vault. Do not prompt for a hash.
    basic_auth_hash_or_password = resolve_value(
        name="basic_auth_hash",
        arg_value=args.basic_auth_hash,
        env_names=["BASIC_AUTH_HASH"],
        kv_name=kv_name_for_secrets,
        kv_secret_name=args.basic_auth_hash_secret,
        interactive=False,
        secret=True,
        prompt_label=None,
        persist_to_kv=persist_to_kv,
    )

    basic_auth_hash: str | None = None
    if basic_auth_hash_or_password:
        if looks_like_bcrypt_hash(basic_auth_hash_or_password):
            basic_auth_hash = basic_auth_hash_or_password
        else:
            # Treat as plaintext password and compute bcrypt hash.
            try:
                basic_auth_hash = bcrypt_hash_password(basic_auth_hash_or_password, cost=args.bcrypt_cost)
            except Exception as e:
                raise SystemExit(f"Failed to compute bcrypt hash for password provided via --basic-auth-hash: {e}")

    if not basic_auth_hash:
        # Ask for password and compute the bcrypt hash (Caddy-compatible).
        basic_auth_password = resolve_value(
            name="basic_auth_password",
            arg_value=args.basic_auth_password,
            env_names=["BASIC_AUTH_PASSWORD"],
            kv_name=kv_name_for_secrets,
            kv_secret_name=None,
            interactive=interactive,
            secret=True,
            prompt_label="Basic Auth password",
            persist_to_kv=False,
        )
        if not basic_auth_password:
            raise SystemExit(
                "Missing Basic Auth password. Provide --basic-auth-password (or BASIC_AUTH_PASSWORD), or store Key Vault secret 'basic-auth-hash'."
            )

        try:
            basic_auth_hash = bcrypt_hash_password(basic_auth_password, cost=args.bcrypt_cost)
        except Exception as e:
            raise SystemExit(f"Failed to compute bcrypt hash for password: {e}")

        # Offer to persist the computed hash.
        if persist_to_kv and args.basic_auth_hash_secret and interactive:
            if prompt_yes_no(
                f"Save computed bcrypt hash to Key Vault secret '{args.basic_auth_hash_secret}'?",
                default=True,
            ):
                try:
                    kv_secret_set(kv_name, args.basic_auth_hash_secret, basic_auth_hash)
                except subprocess.CalledProcessError as e:
                    print(
                        "WARNING: Failed to save computed hash to Key Vault; continuing without persisting.",
                        file=sys.stderr,
                    )
                    print(
                        _format_keyvault_set_help(vault_name=kv_name, stderr=getattr(e, "stderr", None)),
                        file=sys.stderr,
                    )

    # Optional registry credentials for private images (e.g. GHCR).
    # If deploying a public image, do not prompt for registry settings.
    ghcr_private = truthy(os.getenv("GHCR_PRIVATE"))

    # If the image is private and the user didn't specify any build/push flags,
    # default to publishing the image so a single command works end-to-end.
    if not (args.build or args.push or args.build_push):
        # Default to build-push unless explicitly disabled via --no-publish
        publish_default = True
        publish = publish_default if args.publish is None else bool(args.publish)
        if publish:
            build_requested = True
            push_requested = True

    registry_username_seed = (args.registry_username or os.getenv("REGISTRY_USERNAME") or os.getenv("GHCR_USERNAME") or "").strip()
    registry_password_seed = (args.registry_password or os.getenv("REGISTRY_PASSWORD") or os.getenv("GHCR_TOKEN") or "").strip()
    registry_server_seed = (args.registry_server or os.getenv("REGISTRY_SERVER") or "").strip()

    wants_registry_creds = ghcr_private or push_requested or any(
        [
            bool(registry_username_seed),
            bool(registry_password_seed),
            bool(registry_server_seed),
            args.registry_server is not None,
            args.registry_username is not None,
            args.registry_password is not None,
        ]
    )

    registry_server: str | None = None
    registry_username: str | None = None
    registry_password: str | None = None

    if wants_registry_creds:
        inferred_registry, _ = parse_image_ref(image)
        inferred_registry = inferred_registry or ("ghcr.io" if image.startswith("ghcr.io/") else None)

        # When pulling from a private GHCR image, infer the registry server from the image
        # so the user does not get prompted for it.
        registry_server_arg = args.registry_server
        if registry_server_arg is None and not registry_server_seed and (ghcr_private or push_requested) and inferred_registry:
            registry_server_arg = inferred_registry

        registry_server = resolve_value(
            name="registry_server",
            arg_value=registry_server_arg,
            env_names=["REGISTRY_SERVER"],
            kv_name=kv_name_for_secrets,
            kv_secret_name=args.registry_server_secret,
            interactive=interactive,
            secret=False,
            prompt_label="Registry server",
            default="ghcr.io" if image.startswith("ghcr.io/") else None,
            persist_to_kv=persist_to_kv,
        )

        # Infer username for GHCR if not provided
        registry_username_default = None
        if not args.registry_username and not registry_username_seed and image.startswith("ghcr.io/"):
            try:
                # ghcr.io/owner/repo
                registry_username_default = image.split("/")[1]
            except IndexError:
                pass

        registry_username = resolve_value(
            name="registry_username",
            arg_value=args.registry_username,
            env_names=["REGISTRY_USERNAME", "GHCR_USERNAME"],
            kv_name=kv_name_for_secrets,
            kv_secret_name=args.registry_username_secret,
            interactive=interactive,
            secret=False,
            prompt_label="Registry username",
            persist_to_kv=persist_to_kv,
            default=registry_username_default,
        )

        registry_password = resolve_value(
            name="registry_password",
            arg_value=args.registry_password,
            env_names=["REGISTRY_PASSWORD", "GHCR_TOKEN"],
            kv_name=kv_name_for_secrets,
            kv_secret_name=args.registry_password_secret,
            interactive=interactive,
            secret=True,
            prompt_label="Registry token/password",
            persist_to_kv=persist_to_kv,
        )

        if ghcr_private and not (registry_server and registry_username and registry_password):
            raise SystemExit(
                "GHCR_PRIVATE=true but registry credentials are incomplete. Set GHCR_USERNAME/GHCR_TOKEN (and optionally REGISTRY_SERVER=ghcr.io) or pass --registry-*."
            )

    # If we are pushing, ensure we have registry info/creds.
    if push_requested:
        inferred_registry, _ = parse_image_ref(image)
        inferred_registry = inferred_registry or ("ghcr.io" if image.startswith("ghcr.io/") else None)

        # If user didn't set REGISTRY_SERVER, infer it from image.
        if not registry_server and inferred_registry:
            registry_server = inferred_registry

        if not registry_server:
            raise SystemExit(
                "Cannot determine registry server for push. Set REGISTRY_SERVER (e.g. ghcr.io) or pass --registry-server."
            )

        # For pushes, credentials are required even if the image is public.
        if not (registry_username and registry_password):
            raise SystemExit(
                "--push/--build-push requires registry credentials. Set GHCR_USERNAME/GHCR_TOKEN (or REGISTRY_USERNAME/REGISTRY_PASSWORD) or pass --registry-username/--registry-password."
            )

    # Build/push before deploy if requested.
    if build_requested or push_requested:
        # Docker operations happen from the repo root by default.
        docker_context = (args.docker_context or str(repo_root)).strip() or str(repo_root)
        dockerfile = (args.dockerfile or "").strip() or None

        if not dockerfile and not args.docker_context:
            if (repo_root / "docker" / "Dockerfile").exists():
                # If docker/Dockerfile exists and no context given,
                # assume the user wants to build the inner "docker" directory as a context.
                docker_context = str(repo_root / "docker")
                # Leave dockerfile=None so it defaults to "Dockerfile" inside that context.
                dockerfile = None

        if build_requested:
            print(f"[docker] building image: {image}")
            try:
                docker_build(image=image, context_dir=docker_context, dockerfile=dockerfile)
            except FileNotFoundError:
                raise SystemExit("Docker not found. Install Docker and ensure 'docker' is on PATH.")

        if push_requested:
            assert registry_server and registry_username and registry_password
            try:
                docker_login(registry=registry_server, username=registry_username, token=registry_password)
                print(f"[docker] pushing image: {image}")
                try:
                    docker_push(image=image)
                except subprocess.CalledProcessError as e:
                    hint = _hint_for_ghcr_scope_error(getattr(e, "stderr", None))
                    if hint:
                        print(hint, file=sys.stderr)
                    raise
            except FileNotFoundError:
                raise SystemExit("Docker not found. Install Docker and ensure 'docker' is on PATH.")

    # Normalize: if no username/password provided, treat registry creds as disabled.
    # This prevents a defaulted registry_server (e.g. ghcr.io) from triggering the
    # "partial credentials" failure for public images.
    if not registry_username and not registry_password:
        registry_server = None

    if any([registry_server, registry_username, registry_password]) and not all([registry_server, registry_username, registry_password]):
        raise SystemExit(
            "Partial registry credentials provided. You must set all of registry server/username/password or none (for public images)."
        )

    storage_key = get_storage_key(storage_name, rg)
    identity_id, identity_client_id, identity_tenant_id = get_identity_details(identity_name, rg)

    # Recreate container group for identity/env updates.
    # Delete existing container if any, then wait for Azure to fully clean up to prevent "Conflict" errors.
    run_az_command(["container", "delete", "--resource-group", rg, "--name", name, "--yes"], capture_output=False, ignore_errors=True)
    
    # Wait for container to be fully deleted (not just deletion initiated)
    print("[deploy] Waiting for previous container to be fully deleted...")
    max_wait = 120  # seconds
    poll_interval = 5
    waited = 0
    while waited < max_wait:
        # Check if container still exists
        result = run_az_command(
            ["container", "show", "--resource-group", rg, "--name", name, "--query", "provisioningState", "-o", "tsv"],
            capture_output=True,
            ignore_errors=True,
            verbose=False,
        )
        if result is None:
            # Container no longer exists
            print(f"[deploy] Previous container deleted after {waited}s")
            break
        state = str(result).strip().lower()
        if state in ("deleting", "pending"):
            print(f"[deploy] Container still {state}... waiting")
        time.sleep(poll_interval)
        waited += poll_interval
    else:
        print(f"[deploy] Warning: Timed out waiting for container deletion after {max_wait}s, proceeding anyway...")

    caddy_image = (args.caddy_image or "").strip() or "caddy:2-alpine"

    if args.prefetch_images:
        try:
            print(f"[docker] prefetching caddy image: {caddy_image}")
            docker_pull(image=caddy_image)

            # If we are using GHCR for the main image, mirror Caddy to GHCR as well to avoid
            # multi-registry conflicts (ACI "RegistryErrorResponse" from Docker Hub).
            # We assume if the user is pushing/using 'ghcr.io', we can also push caddy there.
            if registry_server and "ghcr.io" in registry_server and registry_username:
                # Target: ghcr.io/<owner>/caddy:2-alpine
                # We need the owner from the registry_username or implied from the main image
                caddy_mirror_tag = f"{registry_server}/{registry_username}/caddy:2-alpine"
                
                print(f"[docker] Mirroring caddy to GHCR: {caddy_mirror_tag}")
                try:
                    # Retag
                    subprocess.run(["docker", "tag", caddy_image, caddy_mirror_tag], check=True, capture_output=True)
                    # Push
                    docker_push(image=caddy_mirror_tag)
                    # Use the mirrored image in the YAML
                    caddy_image = caddy_mirror_tag
                    print(f"[docker] Successfully mirrored caddy. Using: {caddy_image}")
                except Exception as e:
                    print(f"[warn] Failed to mirror caddy to GHCR ({e}); falling back to {caddy_image}", file=sys.stderr)

        except Exception as e:
            print(f"[warn] Could not prefetch caddy image locally ({e}); continuing.", file=sys.stderr)

    yaml_text = generate_deploy_yaml(
        name=name,
        location=location,
        image=image,
        registry_server=registry_server,
        registry_username=registry_username,
        registry_password=registry_password,
        identity_id=identity_id,
        identity_client_id=identity_client_id,
        identity_tenant_id=identity_tenant_id,
        storage_name=storage_name,
        storage_key=storage_key,
        kv_name=kv_name,
        dns_label=dns_label,
        public_domain=public_domain,
        acme_email=acme_email,
        basic_auth_user=basic_auth_user,
        basic_auth_hash=basic_auth_hash,
        cpu_cores=args.cpu,
        memory_gb=args.memory,
        share_workspace=share_workspace,
        caddy_data_share_name=caddy_data_share,
        caddy_config_share_name=caddy_config_share,
        caddy_image=caddy_image,
    )

    with tempfile.NamedTemporaryFile("w", suffix=".yaml", delete=False) as f:
        f.write(yaml_text)
        yaml_path = f.name

    print(f"[deploy] wrote: {yaml_path}")

    # Retry container creation with exponential backoff for transient registry errors
    max_retries = 5
    base_delay = 10.0  # seconds
    for attempt in range(1, max_retries + 1):
        try:
            run_az_command(["container", "create", "--resource-group", rg, "--file", yaml_path], capture_output=False)
            break  # Success
        except subprocess.CalledProcessError as e:
            err = getattr(e, "stderr", "") or ""
            # Check if it's a transient registry conflict error or generic registry error
            # Examples:
            # - 'Conflict':'RegistryErrorResponse'
            # - (RegistryErrorResponse) An error response is received from the docker registry
            is_transient = "RegistryErrorResponse" in err or "Conflict" in err
            if is_transient and attempt < max_retries:
                sleep_time = min(60.0, base_delay * (2 ** (attempt - 1)))
                print(f"[deploy] Registry conflict error (attempt {attempt}/{max_retries}). Retrying in {sleep_time:.0f}s...")
                time.sleep(sleep_time)
            else:
                # Not a transient error or out of retries
                raise

    print("\n[done] Deployed.")
    print(f"  FQDN: {dns_label}.{location}.azurecontainer.io")
    print(f"  https://{public_domain}/  (VS Code)")



if __name__ == "__main__":
    raise SystemExit(main())
